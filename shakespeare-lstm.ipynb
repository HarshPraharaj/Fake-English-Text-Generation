{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The Notebook is laid out as follows:\n### 1) Loading and cleaning the Dataset\n### 2) Pre Processing Data\n### 3) Tokenization\n### 4) Building the LSTM Model\n### 5) Generating fake text from the model\n### 6) Conclusion\n\n##### 7) Trying out GPT2-HugginFace(Still in work)"},{"metadata":{},"cell_type":"markdown","source":"### Installing dependencies"},{"metadata":{"id":"LVB0ac_jp4G1","trusted":true},"cell_type":"code","source":"import os\nimport tensorflow as tf\nimport requests\nimport string\n\nimport numpy as np\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential,Model\nfrom keras.layers import Dense,Flatten,LSTM,Embedding,Dropout,Bidirectional\n","execution_count":1,"outputs":[]},{"metadata":{"id":"uPDFlE3Zqv1w"},"cell_type":"markdown","source":"## Loading Dataset"},{"metadata":{"id":"1cP3UQ5euSeh","outputId":"de2bcc27-e68d-47a8-c1cc-3fe1f3e42c4f","trusted":true},"cell_type":"code","source":"response = requests.get('https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt')\n#response.text","execution_count":42,"outputs":[]},{"metadata":{"id":"fTDtM_BQygKI"},"cell_type":"markdown","source":"Removing all (\\n)'s"},{"metadata":{"id":"-xPDn36YxUcU","outputId":"a16ad811-b41b-42f8-ea1c-6f7a9f08be8f","trusted":true},"cell_type":"code","source":"text = response.text.split('\\n')\nprint(\"Number of lines {}\".format(len(text)))","execution_count":3,"outputs":[{"output_type":"stream","text":"Number of lines 167205\n","name":"stdout"}]},{"metadata":{"id":"z2TVn03GyW6L"},"cell_type":"markdown","source":"Creating a single variable with all lines\n"},{"metadata":{"id":"RpQYYl3kxZoo","outputId":"31a1dc4e-d292-47a9-ece3-e0e5c42b863a","trusted":true},"cell_type":"code","source":"text = ' '.join(text)\nprint(text[:250])","execution_count":4,"outputs":[{"output_type":"stream","text":"First Citizen: Before we proceed any further, hear me speak.  All: Speak, speak.  First Citizen: You are all resolved rather to die than to famish?  All: Resolved. resolved.  First Citizen: First, you know Caius Marcius is chief enemy to the people. \n","name":"stdout"}]},{"metadata":{"id":"u3sHsewJy92m"},"cell_type":"markdown","source":"## Pre-Processing data"},{"metadata":{"id":"hkuh1pyXy7SX","trusted":true},"cell_type":"code","source":"def text_processing(text):\n  \n    \"\"\" Pre Processing the data \n\n    Args: \n      text: Sequence of data to be pre-processed  \n\n    Returns:\n      tokens: Returns cleaned data that is aplhanumeric and in lower case.\n    \"\"\"\n\n\n  tokens = text.split()\n  table = str.maketrans('','',string.punctuation)\n  tokens = [w.translate(table) for w in tokens]\n  tokens = [word for word in tokens if word.isalpha()]\n  tokens = [word.lower() for word in tokens]\n\n  return tokens\n","execution_count":5,"outputs":[]},{"metadata":{"id":"TffgoFet2mD5","outputId":"e34651ba-f16e-427e-9d8b-10c10c41512e","trusted":true},"cell_type":"code","source":"tokens = text_processing(text)\ntokens[:10]\n","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"['first',\n 'citizen',\n 'before',\n 'we',\n 'proceed',\n 'any',\n 'further',\n 'hear',\n 'me',\n 'speak']"},"metadata":{}}]},{"metadata":{"id":"ATszXuSj27wK","outputId":"b613d2f0-744a-4531-8ceb-bb7cf8baa868","trusted":true},"cell_type":"code","source":"prev_seq = 50 + 1 #Number of previous words the model will consider to predict the next word\nsentances = []\nfor i in range(0,len(tokens)):\n  seq = tokens[i:i+prev_seq] #Creating sequences of 50 words each\n  sentance = ' '.join(seq) # Joining tokens of same sequence\n  sentances.append(sentance) #Appending all sentances to one list\n\n  if i > 200000:\n    break\nprint(len(sentances))\nsentances[0]","execution_count":7,"outputs":[{"output_type":"stream","text":"200002\n","name":"stdout"},{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"'first citizen before we proceed any further hear me speak all speak speak first citizen you are all resolved rather to die than to famish all resolved resolved first citizen first you know caius marcius is chief enemy to the people all we knowt we knowt first citizen let us kill'"},"metadata":{}}]},{"metadata":{"id":"t3jAmBpT5sXC"},"cell_type":"markdown","source":"## Tokenization"},{"metadata":{"id":"UXSXvs4R4_-Z","trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(sentances) # Every unique char will be assigned an integer \nsequence = np.array(tokenizer.texts_to_sequences(sentances)) # Replaces characters with their tokenizer output from above function","execution_count":8,"outputs":[]},{"metadata":{"id":"dbrV84NV_dHQ","outputId":"1c7e6db2-c0f1-4961-8673-7ecd83867d90","trusted":true},"cell_type":"code","source":"vocab = tokenizer.word_index #Gives all Word:Index mapping\nvocab_length = len(vocab) + 1\nprint(vocab_length)\n","execution_count":9,"outputs":[{"output_type":"stream","text":"12750\n","name":"stdout"}]},{"metadata":{"id":"i24wczxY5k77","trusted":true},"cell_type":"code","source":"X = sequence[:,:-1]  # All columns except the last (50 prev words to predict 51st(Seq length was 51))\ny = sequence[:,-1]  #Select the last columns only(Word to be predicted)","execution_count":10,"outputs":[]},{"metadata":{"id":"Z8e7K-X-79nm","outputId":"9bd537ab-09f6-48e1-ae54-0685f8c717ae","trusted":true},"cell_type":"code","source":"y = to_categorical(y,num_classes=vocab_length)\nseq_length = X.shape[1]\nprint(seq_length)","execution_count":11,"outputs":[{"output_type":"stream","text":"50\n","name":"stdout"}]},{"metadata":{"id":"Eivf431KBnrh"},"cell_type":"markdown","source":"## Building RNN"},{"metadata":{"id":"fSHHPswcBWze","trusted":true},"cell_type":"code","source":"def create_model(vocab_length,seq_length):\n    \n    \"\"\" Creating the LSTM based RNN\n\n    Args: \n      vocab_length: Lenght of the vocabulary(Unique word : index mapping)\n      seq_length: Length of each sample\n\n    Returns:\n      model: Model\n    \"\"\"\n\n    model = Sequential()\n    model.add(Embedding(vocab_length,50,input_length=seq_length))\n    model.add((LSTM(100,return_sequences = True)))\n    model.add((LSTM(100)))\n    #model.add(Dropout(0.1))\n    model.add(Dense(100,activation='relu'))\n    model.add(Dense(vocab_length,activation='softmax'))\n\n    return model\n","execution_count":12,"outputs":[]},{"metadata":{"id":"EYwNa4TEChp6","outputId":"afb6281f-4aa6-4137-e4bd-8d4f370f7028","trusted":true},"cell_type":"code","source":"model = create_model(vocab_length,seq_length)\nmodel.summary()","execution_count":13,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 50, 50)            637500    \n_________________________________________________________________\nlstm (LSTM)                  (None, 50, 100)           60400     \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 100)               80400     \n_________________________________________________________________\ndense (Dense)                (None, 100)               10100     \n_________________________________________________________________\ndense_1 (Dense)              (None, 12750)             1287750   \n=================================================================\nTotal params: 2,076,150\nTrainable params: 2,076,150\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"id":"wxyxoOi2Cjge","trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',optimizer = 'adam',metrics=['accuracy'])\n","execution_count":14,"outputs":[]},{"metadata":{"id":"wMEg45gXCvK0","outputId":"5ad4a19e-3702-4244-f592-61b1556060ec","trusted":true},"cell_type":"code","source":"model.fit(X,y,batch_size = 256,epochs = 100)","execution_count":15,"outputs":[{"output_type":"stream","text":"Epoch 1/100\n782/782 [==============================] - 25s 32ms/step - loss: 6.8737 - accuracy: 0.0312\nEpoch 2/100\n782/782 [==============================] - 24s 31ms/step - loss: 6.5192 - accuracy: 0.0406\nEpoch 3/100\n782/782 [==============================] - 24s 31ms/step - loss: 6.3209 - accuracy: 0.0573\nEpoch 4/100\n782/782 [==============================] - 24s 30ms/step - loss: 6.1362 - accuracy: 0.0720\nEpoch 5/100\n782/782 [==============================] - 25s 31ms/step - loss: 6.0026 - accuracy: 0.0813\nEpoch 6/100\n782/782 [==============================] - 25s 31ms/step - loss: 5.8913 - accuracy: 0.0868\nEpoch 7/100\n782/782 [==============================] - 24s 31ms/step - loss: 5.7965 - accuracy: 0.0911\nEpoch 8/100\n782/782 [==============================] - 25s 32ms/step - loss: 5.7127 - accuracy: 0.0945\nEpoch 9/100\n782/782 [==============================] - 24s 31ms/step - loss: 5.6338 - accuracy: 0.0966\nEpoch 10/100\n782/782 [==============================] - 24s 30ms/step - loss: 5.5554 - accuracy: 0.0998\nEpoch 11/100\n782/782 [==============================] - 24s 30ms/step - loss: 5.4761 - accuracy: 0.1026\nEpoch 12/100\n782/782 [==============================] - 23s 30ms/step - loss: 5.3994 - accuracy: 0.1052\nEpoch 13/100\n782/782 [==============================] - 24s 31ms/step - loss: 5.3247 - accuracy: 0.1076\nEpoch 14/100\n782/782 [==============================] - 23s 30ms/step - loss: 5.2538 - accuracy: 0.1093\nEpoch 15/100\n782/782 [==============================] - 24s 31ms/step - loss: 5.1851 - accuracy: 0.1120\nEpoch 16/100\n782/782 [==============================] - 24s 30ms/step - loss: 5.1184 - accuracy: 0.1140\nEpoch 17/100\n782/782 [==============================] - 24s 31ms/step - loss: 5.0553 - accuracy: 0.1159\nEpoch 18/100\n782/782 [==============================] - 24s 31ms/step - loss: 4.9954 - accuracy: 0.1185\nEpoch 19/100\n782/782 [==============================] - 23s 30ms/step - loss: 4.9393 - accuracy: 0.1222\nEpoch 20/100\n782/782 [==============================] - 24s 31ms/step - loss: 4.8855 - accuracy: 0.1251\nEpoch 21/100\n782/782 [==============================] - 24s 31ms/step - loss: 4.8366 - accuracy: 0.1290\nEpoch 22/100\n782/782 [==============================] - 23s 30ms/step - loss: 4.7906 - accuracy: 0.1330\nEpoch 23/100\n782/782 [==============================] - 24s 31ms/step - loss: 4.7458 - accuracy: 0.1370\nEpoch 24/100\n782/782 [==============================] - 23s 30ms/step - loss: 4.7061 - accuracy: 0.1397\nEpoch 25/100\n782/782 [==============================] - 24s 31ms/step - loss: 4.6656 - accuracy: 0.1438\nEpoch 26/100\n782/782 [==============================] - 23s 30ms/step - loss: 4.6281 - accuracy: 0.1471\nEpoch 27/100\n782/782 [==============================] - 23s 29ms/step - loss: 4.5937 - accuracy: 0.1512\nEpoch 28/100\n782/782 [==============================] - 24s 30ms/step - loss: 4.5578 - accuracy: 0.1545\nEpoch 29/100\n782/782 [==============================] - 23s 30ms/step - loss: 4.5266 - accuracy: 0.1578\nEpoch 30/100\n782/782 [==============================] - 24s 31ms/step - loss: 4.4940 - accuracy: 0.1613\nEpoch 31/100\n782/782 [==============================] - 24s 31ms/step - loss: 4.4628 - accuracy: 0.1642\nEpoch 32/100\n782/782 [==============================] - 24s 30ms/step - loss: 4.4335 - accuracy: 0.1665\nEpoch 33/100\n782/782 [==============================] - 24s 31ms/step - loss: 4.4027 - accuracy: 0.1705\nEpoch 34/100\n782/782 [==============================] - 24s 30ms/step - loss: 4.3747 - accuracy: 0.1738\nEpoch 35/100\n782/782 [==============================] - 23s 29ms/step - loss: 4.3463 - accuracy: 0.1762\nEpoch 36/100\n782/782 [==============================] - 23s 30ms/step - loss: 4.3201 - accuracy: 0.1793\nEpoch 37/100\n782/782 [==============================] - 23s 30ms/step - loss: 4.2937 - accuracy: 0.1823\nEpoch 38/100\n782/782 [==============================] - 24s 30ms/step - loss: 4.2663 - accuracy: 0.1852\nEpoch 39/100\n782/782 [==============================] - 23s 29ms/step - loss: 4.2423 - accuracy: 0.1881\nEpoch 40/100\n782/782 [==============================] - 23s 30ms/step - loss: 4.2169 - accuracy: 0.1917\nEpoch 41/100\n782/782 [==============================] - 23s 30ms/step - loss: 4.1923 - accuracy: 0.1938\nEpoch 42/100\n782/782 [==============================] - 23s 29ms/step - loss: 4.1672 - accuracy: 0.1968\nEpoch 43/100\n782/782 [==============================] - 23s 30ms/step - loss: 4.1440 - accuracy: 0.2001\nEpoch 44/100\n782/782 [==============================] - 23s 29ms/step - loss: 4.1208 - accuracy: 0.2032\nEpoch 45/100\n782/782 [==============================] - 23s 30ms/step - loss: 4.0983 - accuracy: 0.2059\nEpoch 46/100\n782/782 [==============================] - 24s 30ms/step - loss: 4.0760 - accuracy: 0.2086\nEpoch 47/100\n782/782 [==============================] - 23s 30ms/step - loss: 4.0548 - accuracy: 0.2105\nEpoch 48/100\n782/782 [==============================] - 23s 30ms/step - loss: 4.0321 - accuracy: 0.2138\nEpoch 49/100\n782/782 [==============================] - 24s 30ms/step - loss: 4.0097 - accuracy: 0.2158\nEpoch 50/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.9906 - accuracy: 0.2184\nEpoch 51/100\n782/782 [==============================] - 24s 31ms/step - loss: 3.9697 - accuracy: 0.2216\nEpoch 52/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.9496 - accuracy: 0.2239\nEpoch 53/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.9293 - accuracy: 0.2262\nEpoch 54/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.9103 - accuracy: 0.2296\nEpoch 55/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.8899 - accuracy: 0.2319\nEpoch 56/100\n782/782 [==============================] - 24s 30ms/step - loss: 3.8709 - accuracy: 0.2349\nEpoch 57/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.8520 - accuracy: 0.2369\nEpoch 58/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.8342 - accuracy: 0.2396\nEpoch 59/100\n782/782 [==============================] - 24s 30ms/step - loss: 3.8162 - accuracy: 0.2427\nEpoch 60/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.7978 - accuracy: 0.2450\nEpoch 61/100\n782/782 [==============================] - 24s 30ms/step - loss: 3.7800 - accuracy: 0.2470\nEpoch 62/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.7629 - accuracy: 0.2492\nEpoch 63/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.7449 - accuracy: 0.2517\nEpoch 64/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.7297 - accuracy: 0.2540\nEpoch 65/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.7113 - accuracy: 0.2567\nEpoch 66/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.6979 - accuracy: 0.2593\nEpoch 67/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.6800 - accuracy: 0.2612\nEpoch 68/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.6628 - accuracy: 0.2633\nEpoch 69/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.6465 - accuracy: 0.2661\nEpoch 70/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.6331 - accuracy: 0.2688\nEpoch 71/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.6176 - accuracy: 0.2715\nEpoch 72/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.5992 - accuracy: 0.2730\nEpoch 73/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.5844 - accuracy: 0.2756\nEpoch 74/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.5713 - accuracy: 0.2774\nEpoch 75/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.5579 - accuracy: 0.2794\nEpoch 76/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.5396 - accuracy: 0.2823\nEpoch 77/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.5262 - accuracy: 0.2841\nEpoch 78/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.5118 - accuracy: 0.2865\nEpoch 79/100\n","name":"stdout"},{"output_type":"stream","text":"782/782 [==============================] - 23s 29ms/step - loss: 3.4994 - accuracy: 0.2887\nEpoch 80/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.4858 - accuracy: 0.2899\nEpoch 81/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.4716 - accuracy: 0.2926\nEpoch 82/100\n782/782 [==============================] - 24s 30ms/step - loss: 3.4576 - accuracy: 0.2942\nEpoch 83/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.4429 - accuracy: 0.2968\nEpoch 84/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.4340 - accuracy: 0.2976\nEpoch 85/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.4166 - accuracy: 0.2993\nEpoch 86/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.4026 - accuracy: 0.3024\nEpoch 87/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.3911 - accuracy: 0.3043\nEpoch 88/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.3796 - accuracy: 0.3055\nEpoch 89/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.3644 - accuracy: 0.3083\nEpoch 90/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.3507 - accuracy: 0.3102\nEpoch 91/100\n782/782 [==============================] - 22s 29ms/step - loss: 3.3387 - accuracy: 0.3118\nEpoch 92/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.3293 - accuracy: 0.3131\nEpoch 93/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.3132 - accuracy: 0.3166\nEpoch 94/100\n782/782 [==============================] - 22s 29ms/step - loss: 3.3033 - accuracy: 0.3175\nEpoch 95/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.2876 - accuracy: 0.3202\nEpoch 96/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.2792 - accuracy: 0.3219\nEpoch 97/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.2650 - accuracy: 0.3239\nEpoch 98/100\n782/782 [==============================] - 23s 30ms/step - loss: 3.2525 - accuracy: 0.3253\nEpoch 99/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.2434 - accuracy: 0.3257\nEpoch 100/100\n782/782 [==============================] - 23s 29ms/step - loss: 3.2333 - accuracy: 0.3286\n","name":"stdout"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f4e52d9a510>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Generating Fake Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_text(model,tokenizer,text_seq_length,seed_text,n_words):\n    \n    \n    \"\"\" Generating Fake text using the model trained\n\n    Args:\n      model: RNN trained \n      tokenizer: Tokenizer object for preprocessing\n      text_seq_length: Length of each sample\n      seed_text: starting text based on which model will generate new samples\n      n_words: Number of sample to generate\n\n    Returns:\n      model: Returns sequence of 100 predicted words\n    \"\"\"\n\n\n    pred_text = []\n    \n    for _ in range(n_words):\n        \n        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n        encoded = pad_sequences([encoded],maxlen = 50,truncating = 'pre')\n        \n        y_pred = model.predict_classes(encoded)\n        print(y_pred)\n        #print(y_pred)\n        \n        word_y = ''\n        for word,index in tokenizer.word_index.items():\n            #print(index,y_pred)\n            if index == y_pred:\n                word_y = word\n                #print(word_y)\n                break\n            \n        seed_text = seed_text + ' ' + word_y\n        pred_text.append(word_y)\n        #print(pred_text)\n    return ' '.join(pred_text)\n            \n        ","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generating new text after the first 50 words"},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_text(model,tokenizer,seq_length,sentances[0],100)","execution_count":17,"outputs":[{"output_type":"stream","text":"[26]\n[4760]\n[33]\n[257]\n[7]\n[44]\n[4]\n[68]\n[8]\n[234]\n[5]\n[107]\n[2]\n[208]\n[20]\n[4]\n[68]\n[10522]\n[6420]\n[13]\n[6]\n[11]\n[9078]\n[84]\n[12]\n[17]\n[140]\n[714]\n[20]\n[6]\n[36]\n[202]\n[1]\n[3154]\n[5]\n[1]\n[1948]\n[1]\n[569]\n[4]\n[23]\n[8]\n[234]\n[540]\n[395]\n[4]\n[30]\n[12]\n[17]\n[793]\n[3305]\n[273]\n[4]\n[23]\n[147]\n[8]\n[2135]\n[963]\n[3]\n[17]\n[8]\n[12518]\n[2]\n[11886]\n[18]\n[1482]\n[2096]\n[67]\n[123]\n[6]\n[23]\n[6529]\n[6]\n[41]\n[12]\n[538]\n[6]\n[2]\n[4]\n[68]\n[8]\n[234]\n[540]\n[3]\n[17]\n[8]\n[4861]\n[5]\n[18]\n[190]\n[2]\n[81]\n[24]\n[1651]\n[24]\n[1]\n[3879]\n[90]\n[358]\n[60]\n","name":"stdout"},{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"'him mariner no madam my lord i am a gentleman of mine and nothing but i am galledmightst savours for you is weandi must not be thus bold but you shall bear the burden of the sky the heavens i have a gentleman born clown i will not be consul sixth citizen i have been a stranger glad to be a rabbit and paind your unknown sovereignty duke vincentio you have confessed you do not mark you and i am a gentleman born to be a surplus of your grace and yet as glorious as the fliers first murderer how'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Actual text following the first 50 words"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentances[1]","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"'citizen before we proceed any further hear me speak all speak speak first citizen you are all resolved rather to die than to famish all resolved resolved first citizen first you know caius marcius is chief enemy to the people all we knowt we knowt first citizen let us kill him'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n### The output generated by LSTM's are pretty coherent to some extent. \n\n### The drawback of LSTM is that, at each time step we are predicting only one entity. Secondly, the performance of LSTM's worsens as we try to predict larger sequences(Vanishing Gradient).\n\n### Alternatives to LSTM can includes 2D convolutional based neural network with causal convolution and attention based encoder-decoder models which can work for much longer sequences."},{"metadata":{},"cell_type":"markdown","source":"* ---------------------------------------------------------------------------------------------------------------------------------------------- *"},{"metadata":{},"cell_type":"markdown","source":"## Trying out GPT2-HuggingFace"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import TFGPT2LMHeadModel, GPT2Tokenizer","execution_count":20,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\nGPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)\n\n\nGPT2.summary()\n","execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"367bdde601fd4897834c5ae478fdccf2"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9f3e4fceae740f2bf7a870c445f80c2"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=764.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f8ab6d7f6544a198672d42530fc1167"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3096618024.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91ff03af96624ba1b72b157d1c225457"}},"metadata":{}},{"output_type":"stream","text":"\nModel: \"tfgp_t2lm_head_model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntransformer (TFGPT2MainLayer multiple                  774030080 \n=================================================================\nTotal params: 774,030,080\nTrainable params: 774,030,080\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_GPT(model,tokenizer,seed_text,n_words):\n    \n    \n    encoded = tokenizer.encode(seed_text, return_tensors='tf')\n    \n    sample_outputs = model.generate(\n                              encoded,\n                              do_sample = True, \n                              max_length = n_words,\n                              temperature = 0.7,\n                              top_k = 50, \n                              top_p = 0.85 \n                              )\n\n    print(\"Output:\\n\" + 100 * '-')\n    for i, sample_output in enumerate(sample_outputs):\n        print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n        print('')","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_GPT(GPT2,tokenizer,'If Antonio fails to pay Shylock back, Shylock demands a pound of Antonios flesh.',100)","execution_count":41,"outputs":[{"output_type":"stream","text":"Output:\n----------------------------------------------------------------------------------------------------\n0: If Antonio fails to pay Shylock back, Shylock demands a pound of Antonios flesh.\n\nAntonios refuses and Shylock uses his powers to force him to eat the flesh. Shylock then forces the man to confess to the murder of his wife and children. The man is forced to admit that he killed the children because he was angry that the woman he loved was cheating on him. The man then admits to the murder of his wife and children and begs for...\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}